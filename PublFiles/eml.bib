
@Article{	  bellman:2020a,
  author	= {Kerstin Bellman and Nikil Dutt and Lukas Esterle and
		  Andreas Herkersdorf and Axel Jantsch and C. Landauer and P.
		  R. Lewis and M. Platzner and N. TaheriNejad and K.
		  Tammem\"{a}e},
  journal	= {ACM Transactions on Cyber-Physical Systems},
  title		= {Self-aware Cyber-Physical Systems},
  year		= 2020,
  key		= {selfaware,eml},
  pages		= {1-24},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {4},
  issn		= {2378-962X},
  url		= {https://doi.org/10.1145/3375716},
  doi		= {10.1145/3375716},
  abstract	= {In this article, we make the case for the new class of
		  Self-aware Cyber-physical Systems. By bringing together the
		  two established fields of cyber-physical systems and
		  self-aware computing, we aim at creating systems with
		  strongly increased yet managed autonomy, which is a main
		  requirement for many emerging and future applications and
		  technologies. Self-aware cyber-physical systems are
		  situated in a physical environment and constrained in their
		  resources, and they understand their own state and
		  environment and, based on that understanding, are able to
		  make decisions autonomously at runtime in a
		  self-explanatory way. In an attempt to lay out a research
		  agenda, we bring up and elaborate on five key challenges
		  for future self-aware cyber-physical systems: (i) How can
		  we build resource-sensitive yet self-aware systems? (ii)
		  How to acknowledge situatedness and subjectivity? (iii)
		  What are effective infrastructures for implementing
		  self-awareness processes? (iv) How can we verify self-aware
		  cyber-physical systems and, in particular, which guarantees
		  can we give? (v) What novel development processes will be
		  required to engineer self-aware cyber-physical systems? We
		  review each of these challenges in some detail and
		  emphasize that addressing all of them requires the system
		  to make a comprehensive assessment of the situation and a
		  continual introspection of its own state to sensibly
		  balance diverse requirements, constraints, short-term and
		  long-term objectives. Throughout, we draw on three examples
		  of cyber-physical systems that may benefit from
		  self-awareness: a multi-processor system-on-chip, a Mars
		  rover, and an implanted insulin pump. These three very
		  different systems nevertheless have similar
		  characteristics: limited resources, complex unforeseeable
		  environmental dynamics, high expectations on their
		  reliability, and substantial levels of risk associated with
		  malfunctioning. Using these examples, we discuss the
		  potential role of self-awareness in both highly complex and
		  rather more simple systems, and as a main conclusion we
		  highlight the need for research on above listed topics.},
  month		= {June},
  articleno	= {38},
  numpages	= {26},
  keywords	= {Self-awareness, guarantees, development processes,
		  verification, subjectivity, situatedness,
		  resource-sensitive, organizational infrastructure,
		  cyber-physical systems}
}

@InProceedings{	  fruhwirth:2020a,
  title		= {Towards Data-driven Multi-target Tracking for Autonomous
		  Driving},
  abstract	= {We investigate the potential of recurrent neural networks
		  (RNNs) to improve traditional on-line multi-target tracking
		  of traffic participants from an ego-vehicle perspective. To
		  this end, we build a modular tracking framework, based on
		  interacting multiple models (IMM) and unscented Kalman
		  filters (UKF). Following the tracking-by-detection
		  paradigm, we leverage geometric target properties provided
		  by publicly available 3D object detectors. We then train
		  and integrate two RNNs: A state pre-diction network
		  replaces hand-crafted motion mod-els in our filters and a
		  data association network finds detection-to-track
		  assignment probabilities. In our extensive evaluation on
		  the publicly available KITTI dataset we show that our
		  trained models achieve competitive results and are
		  significantly more robust in the case of unreliable object
		  defection.},
  author	= {Christian Fruhwirth-Reisinger and Georg Krispel and Horst
		  Possegger and Horst Bischof},
  year		= {2020},
  key		= {eml},
  language	= {English},
  pages		= {27--36},
  booktitle	= {Proceedings of the 25th Computer Vision Winter Workshop
		  (CVWW)},
  publisher	= {Slovenian Pattern Recognition Society},
  address	= {Slovenia}
}

@Article{	  hoffmann:2020a,
  author	= {Henrik {Hoffmann} and Axel {Jantsch} and Nikil D. {Dutt}},
  journal	= {Proceedings of the IEEE},
  title		= {Embodied Self-Aware Computing Systems},
  year		= 2020,
  pages		= {1-20},
  key		= {selfaware,eml},
  keywords	= {Control;embedded systems (ESs);machine learning
		  (ML);self-aware computing.},
  doi		= {10.1109/JPROC.2020.2977054},
  issn		= {1558-2256},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/HankHoffmann-IEEEProceedings.pdf}
}

@Misc{		  presentation:2020a,
  key		= {ml, eml},
  author	= {Axel Jantsch },
  title		= {Embedded Machine Learning},
  howpublished	= {Invited Presentation at the Summer School on {AI} Enabled
		  Mobility },
  month		= {September},
  year		= 2020,
  annote	= {Graz, Austria},
  url		= {http://jantsch.se/AxelJantsch/presentations/2020/AI-EnabledMobility-SummerSchool.pdf}
}

@misc{presentation:2020b,
  key = {ml, eml},
  author = {Horst Bischof },
  title = {Computer Vision for Detection and Tracking},
  howpublished = {Invited Presentation at the Summer School on {AI} Enabled Mobility },
  month = {September},
  year = 2020,
  annote = {Graz, Austria}
}

@Article{	  taherinejad:2020a,
  author	= {N. {TaheriNejad} and A. {Herkersdorf} and A. {Jantsch}},
  journal	= {IEEE Design Test},
  title		= {Autonomous Systems, Trust and Guarantees},
  year		= 2020,
  issn		= {2168-2356},
  doi		= { 10.1109/MDAT.2020.3024145},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/NimaTaherinejad-DesignAndTest.pdf},
  key		= {selfaware,eml},
  pages		= {1-1}
}

@InProceedings{	  wendt_cognitive_2020,
  address	= {United Kingdom (Online)},
  title		= {Cognitive {Architectures} for {Process} {Monitoring} - an
		  {Analysis}},
  isbn		= {978-1-72814-963-9},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/AlexanderWendt-INDIN.pdf},
  language	= {English},
  booktitle	= {Proceedings of the 18th {IEEE} {International}
		  {Conference} on {Industrial} {Informatics}},
  author	= {Wendt, Alexander and Kollmann, Stefan and Bratukhin,
		  Aleksey and Estaji, Alireza and Sauter, Thilo and Jantsch,
		  Axel},
  month		= jul,
  year		= {2020},
  pages		= {167--173}
}

@InProceedings{	  wendt_speeding_2020,
  address	= {Singapore (Online)},
  title		= {Speeding up {Common} {Hyperparameter} {Optimization}
		  {Methods} by a {Two}-{Phase}-{Search}},
  isbn		= {978-1-72815-413-8},
  abstract	= {Hyperparameter search concerns everybody who works with
		  machine learning. We compare publicly available
		  hyperparameter searches on four datasets. We develop
		  metrics to measure the performance of hyperparameter
		  searches across datasets of different sizes as well as
		  machine learning algorithms. Further, we propose a method
		  of speeding up the search by using subsets of data. Results
		  show that random search performs well compared to Bayesian
		  methods and that a combined search can speed up the search
		  by a factor of 5.},
  language	= {EN},
  booktitle	= {Proceedings of {IECON} 2020 {The} 46th {Annual}
		  {Conference} of the {IEEE} {Industrial} {Electronics}
		  {Society}},
  publisher	= {IEEE IES},
  author	= {Wendt, Alexander and Wuschnig, Marco and Lechner, Martin},
  year		= {2020},
  pages		= {517--522},
  url		= {https://www.researchgate.net/publication/344767663_Speeding_up_Common_Hyperparameter_Optimization_Methods_by_a_Two-Phase-Search}
}

@INPROCEEDINGS{9290876,
  author={S. {Holly} and A. {Wendt} and M. {Lechner}},
  booktitle={2020 11th International Green and Sustainable Computing Workshops (IGSC)}, 
  title={Profiling Energy Consumption of Deep Neural Networks on NVIDIA Jetson Nano}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Improving the capabilities of embedded devices and accelerators for Deep Neural Networks (DNN) leads to a shift from cloud to edge computing. Especially for battery-powered systems, intelligent energy management is critical. In this work, we provide a measurement base for power estimation on NVIDIA Jetson devices. We analyze the effects of different CPU and GPU settings on power consumption, latency, and energy for complete DNNs as well as for individual layers. Furthermore, we provide optimal settings for minimal power and energy consumption for an NVIDIA Jetson Nano.},
  keywords={Power measurement;Sensors;Graphics processing units;Power demand;Neural networks;Hardware;Energy consumption;power;energy;latency;NVIDIA;Jetson;deep neural network;profiling},
  doi={10.1109/IGSC51522.2020.9290876},
  ISSN={},
  month={Oct},}
  
@ARTICLE{wess:2021,
  author={M. {Wess} and M. {Ivanov} and C. {Unger} and A. {Nookala} and A. {Wendt} and A. {Jantsch}},
  journal={IEEE Access}, 
  title={ANNETTE: Accurate Neural Network Execution Time Estimation With Stacked Models}, 
  year={2021},
  volume={9},
  number={},
  pages={3545-3556},
  abstract={With new accelerator hardware for Deep Neural Networks (DNNs), the computing power for Artificial Intelligence (AI) applications has increased rapidly. However, as DNN algorithms become more complex and optimized for specific applications, latency requirements remain challenging, and it is critical to find the optimal points in the design space. To decouple the architectural search from the target hardware, we propose a time estimation framework that allows for modeling the inference latency of DNNs on hardware accelerators based on mapping and layer-wise estimation models. The proposed methodology extracts a set of models from micro-kernel and multi-layer benchmarks and generates a stacked model for mapping and network execution time estimation. We compare estimation accuracy and fidelity of the generated mixed models, statistical models with the roofline model, and a refined roofline model for evaluation. We test the mixed models on the ZCU102 SoC board with Xilinx Deep Neural Network Development Kit (DNNDK) and Intel Neural Compute Stick 2 (NCS2) on a set of 12 state-of-the-art neural networks. It shows an average estimation error of 3.47% for the DNNDK and 7.44% for the NCS2, outperforming the statistical and analytical layer models for almost all selected networks. For a randomly selected subset of 34 networks of the NASBench dataset, the mixed model reaches fidelity of 0.988 in Spearman’s  $\rho $  rank correlation coefficient metric.},
  keywords={Hardware;Computational modeling;Estimation;Benchmark testing;Biological system modeling;Computer architecture;Tools;Analytical models;estimation;neural network hardware},
  doi={10.1109/ACCESS.2020.3047259},
  ISSN={2169-3536},
  month={},}
  
@inproceedings{krispel2020fuseseg,
  title={FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data},
  author={Krispel, Georg and Opitz, Michael and Waltner, Georg and Possegger, Horst and Bischof, Horst},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={1874--1883},
  year={2020}
}

@inproceedings{haas2021shunt,
  title={Neural Network Compression Through Shunt Connections and Knowledge Distillation for Semantic Segmentation Problems},
  author={Bernhard Haas and Alexander Wendt and Axel Jantsch and Matthias Wess},
  booktitle={to be published in Proceedings of AIAI 2021, 17th International Conference on Artificial Intelligence Applications and Innovations},
  volume={1},
  year={2021}
}


@Article{mozelli:2021a,
  author =	 {Amid Mozelli and Nima Taherinejad and Axel Jantsch},
  title =	 {A Study on Confidence: an Unsupervised Multi-Agent
                  Machine Learning Experiment },
  journal =	 {IEEE Design \& Test of Computers },
  year =	 2021,
  key =		 {eml,selfaware},
  note =	 {accepted for publication}
}
