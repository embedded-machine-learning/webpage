
@InProceedings{	  9290876,
  author	= {S. {Holly} and A. {Wendt} and M. {Lechner}},
  booktitle	= {2020 11th International Green and Sustainable Computing
		  Workshops (IGSC)},
  title		= {Profiling Energy Consumption of Deep Neural Networks on
		  NVIDIA Jetson Nano},
  year		= {2020},
  volume	= {},
  number	= {},
  pages		= {1-6},
  abstract	= {Improving the capabilities of embedded devices and
		  accelerators for Deep Neural Networks (DNN) leads to a
		  shift from cloud to edge computing. Especially for
		  battery-powered systems, intelligent energy management is
		  critical. In this work, we provide a measurement base for
		  power estimation on NVIDIA Jetson devices. We analyze the
		  effects of different CPU and GPU settings on power
		  consumption, latency, and energy for complete DNNs as well
		  as for individual layers. Furthermore, we provide optimal
		  settings for minimal power and energy consumption for an
		  NVIDIA Jetson Nano.},
  keywords	= {Power measurement;Sensors;Graphics processing units;Power
		  demand;Neural networks;Hardware;Energy
		  consumption;power;energy;latency;NVIDIA;Jetson;deep neural
		  network;profiling},
  doi		= {10.1109/IGSC51522.2020.9290876},
  issn		= {},
  month		= {Oct}
}

@Article{	  bellman:2020a,
  author	= {Kerstin Bellman and Nikil Dutt and Lukas Esterle and
		  Andreas Herkersdorf and Axel Jantsch and C. Landauer and P.
		  R. Lewis and M. Platzner and N. TaheriNejad and K.
		  Tammem\"{a}e},
  journal	= {ACM Transactions on Cyber-Physical Systems},
  title		= {Self-aware Cyber-Physical Systems},
  year		= 2020,
  key		= {selfaware,eml,cdl},
  pages		= {1-24},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {4},
  issn		= {2378-962X},
  url		= {https://doi.org/10.1145/3375716},
  doi		= {10.1145/3375716},
  abstract	= {In this article, we make the case for the new class of
		  Self-aware Cyber-physical Systems. By bringing together the
		  two established fields of cyber-physical systems and
		  self-aware computing, we aim at creating systems with
		  strongly increased yet managed autonomy, which is a main
		  requirement for many emerging and future applications and
		  technologies. Self-aware cyber-physical systems are
		  situated in a physical environment and constrained in their
		  resources, and they understand their own state and
		  environment and, based on that understanding, are able to
		  make decisions autonomously at runtime in a
		  self-explanatory way. In an attempt to lay out a research
		  agenda, we bring up and elaborate on five key challenges
		  for future self-aware cyber-physical systems: (i) How can
		  we build resource-sensitive yet self-aware systems? (ii)
		  How to acknowledge situatedness and subjectivity? (iii)
		  What are effective infrastructures for implementing
		  self-awareness processes? (iv) How can we verify self-aware
		  cyber-physical systems and, in particular, which guarantees
		  can we give? (v) What novel development processes will be
		  required to engineer self-aware cyber-physical systems? We
		  review each of these challenges in some detail and
		  emphasize that addressing all of them requires the system
		  to make a comprehensive assessment of the situation and a
		  continual introspection of its own state to sensibly
		  balance diverse requirements, constraints, short-term and
		  long-term objectives. Throughout, we draw on three examples
		  of cyber-physical systems that may benefit from
		  self-awareness: a multi-processor system-on-chip, a Mars
		  rover, and an implanted insulin pump. These three very
		  different systems nevertheless have similar
		  characteristics: limited resources, complex unforeseeable
		  environmental dynamics, high expectations on their
		  reliability, and substantial levels of risk associated with
		  malfunctioning. Using these examples, we discuss the
		  potential role of self-awareness in both highly complex and
		  rather more simple systems, and as a main conclusion we
		  highlight the need for research on above listed topics.},
  month		= {June},
  articleno	= {38},
  numpages	= {26},
  keywords	= {Self-awareness, guarantees, development processes,
		  verification, subjectivity, situatedness,
		  resource-sensitive, organizational infrastructure,
		  cyber-physical systems}
}

@InProceedings{	  fruhwirth:2020a,
  title		= {Towards Data-driven Multi-target Tracking for Autonomous
		  Driving},
  abstract	= {We investigate the potential of recurrent neural networks
		  (RNNs) to improve traditional on-line multi-target tracking
		  of traffic participants from an ego-vehicle perspective. To
		  this end, we build a modular tracking framework, based on
		  interacting multiple models (IMM) and unscented Kalman
		  filters (UKF). Following the tracking-by-detection
		  paradigm, we leverage geometric target properties provided
		  by publicly available 3D object detectors. We then train
		  and integrate two RNNs: A state pre-diction network
		  replaces hand-crafted motion mod-els in our filters and a
		  data association network finds detection-to-track
		  assignment probabilities. In our extensive evaluation on
		  the publicly available KITTI dataset we show that our
		  trained models achieve competitive results and are
		  significantly more robust in the case of unreliable object
		  defection.},
  author	= {Christian Fruhwirth-Reisinger and Georg Krispel and Horst
		  Possegger and Horst Bischof},
  year		= {2020},
  key		= {eml,cdl},
  language	= {English},
  pages		= {27--36},
  booktitle	= {Proceedings of the 25th Computer Vision Winter Workshop
		  (CVWW)},
  publisher	= {Slovenian Pattern Recognition Society},
  address	= {Slovenia}
}

@MastersThesis{	  glinserer:2021a,
  author	= {Andreas Glinserer},
  title		= {Autopruning mit {Intel} Distiller und Evaluation auf einem
		  {Jetson} {Xavier} {AGX}},
  school	= {TU Wien},
  year		= 2021,
  key		= {eml,cdl},
  month		= {June}
}

}

@MastersThesis{	  haas:2021b,
  author	= {Bernhard Haas},
  title		= {Compressing MobileNet With Shunt Connections for {NVIDIA}
		  Hardware },
  school	= {TU Wien},
  year		= 2021,
  key		= {eml,cdl},
  url		= {https://publik.tuwien.ac.at/files/publik_295948.pdf}
}

@InProceedings{	  haas_neural_2021,
  address	= {Greece (online)},
  title		= {Neural {Network} {Compression} {Through} {Shunt}
		  {Connections} and {Knowledge} {Distillation} for {Semantic}
		  {Segmentation} {Problems}},
  volume	= {1},
  isbn		= {978-3-030-79149-0},
  doi		= {https://doi.org/10.1007/978-3-030-79150-6},
  language	= {English},
  booktitle	= {Artificial {Intelligence} {Applications} and
		  {Innovations}, 17th {IFIP} {WG} 12.5 {International}
		  {Conference}},
  publisher	= {Springer},
  author	= {Haas, Bernhard and Wendt, Alexander and Jantsch, Axel and
		  Wess, Matthias},
  year		= {2021},
  pages		= {349--361}
}

@Article{	  hoffmann:2020a,
  author	= {Henrik {Hoffmann} and Axel {Jantsch} and Nikil D. {Dutt}},
  journal	= {Proceedings of the IEEE},
  title		= {Embodied Self-Aware Computing Systems},
  year		= 2020,
  pages		= {1-20},
  key		= {selfaware,eml,cdl},
  keywords	= {Control;embedded systems (ESs);machine learning
		  (ML);self-aware computing.},
  doi		= {10.1109/JPROC.2020.2977054},
  issn		= {1558-2256},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/HankHoffmann-IEEEProceedings.pdf}
}

@MastersThesis{	  ivanov:2021a,
  author	= {Matvey Ivanov},
  title		= {Embedded Machine Learning Demonstrator},
  school	= {TU Wien},
  year		= 2021,
  key		= {cdl},
  type		= {Bachelor's Thesis},
  url		= {https://publik.tuwien.ac.at/files/publik_296007.pdf}
}

@MastersThesis{	  kinfu:2020a,
  author	= {Kaleab Alemayehu Kinfu},
  title		= {Lifelong Learning for Autonomous Vehicles: Monocular Depth
		  Estimation },
  school	= {TU Graz},
  year		= 2020,
  key		= {cdl}
}

@InProceedings{	  krispel2020fuseseg,
  title		= {FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal
		  Data},
  author	= {Krispel, Georg and Opitz, Michael and Waltner, Georg and
		  Possegger, Horst and Bischof, Horst},
  booktitle	= {The IEEE Winter Conference on Applications of Computer
		  Vision},
  pages		= {1874--1883},
  year		= {2020}
}

@Article{	  lechner:2021a,
  author	= {Martin Lechner and Axel Jantsch},
  title		= {Blackthorn: Latency Estimation Framework for {CNNs} on
		  Embedded {Nvidia} Platforms},
  journal	= {IEEE Access},
  year		= 2021,
  key		= {eml,cdl},
  doi		= {10.1109/ACCESS.2021.3101936},
  url		= {http://jantsch.se/AxelJantsch/papers/2021/MartinLechner-IEEEAccess.pdf}
}

@Article{	  mozelli:2021a,
  author	= {Amid Mozelli and Nima Taherinejad and Axel Jantsch},
  title		= {A Study on Confidence: an Unsupervised Multi-Agent Machine
		  Learning Experiment },
  journal	= {IEEE Design \& Test of Computers },
  year		= 2021,
  key		= {eml,cdl,selfaware},
  issn		= { 2168-2356 },
  doi		= { 10.1109/MDAT.2021.3078341 }
}

@PhDThesis{	  opitz:2021a,
  author	= {Michael Opitz},
  title		= {Efficient Ensembles for Deep Learning},
  school	= {TU Graz},
  year		= 2021,
  key		= {cdl}
}

@Misc{		  presentation:2020a,
  key		= {ml, eml,cdl},
  author	= {Axel Jantsch },
  title		= {Embedded Machine Learning},
  howpublished	= {Invited Presentation at the Summer School on {AI} Enabled
		  Mobility },
  month		= {September},
  year		= 2020,
  annote	= {Graz, Austria},
  url		= {http://jantsch.se/AxelJantsch/presentations/2020/AI-EnabledMobility-SummerSchool.pdf}
}

@Misc{		  presentation:2020b,
  key		= {ml, eml},
  author	= {Horst Bischof },
  title		= {Computer Vision for Detection and Tracking},
  howpublished	= {Invited Presentation at the Summer School on {AI} Enabled
		  Mobility },
  month		= {September},
  year		= 2020,
  annote	= {Graz, Austria}
}

@MastersThesis{	  roth:2020a,
  author	= {Julian Roth},
  title		= {Auswertung von Cloudbasierten Machine Learning Frameworks
		  für Supervised Machine Learning},
  school	= {TU Wien},
  year		= 2020,
  type		= {Bachelor's Thesis},
  key		= {cdl},
  url		= {https://publik.tuwien.ac.at/files/publik_295897.pdf}
}

@Article{	  taherinejad:2020a,
  author	= {N. {TaheriNejad} and A. {Herkersdorf} and A. {Jantsch}},
  journal	= {IEEE Design Test},
  title		= {Autonomous Systems, Trust and Guarantees},
  year		= 2020,
  issn		= {2168-2356},
  doi		= { 10.1109/MDAT.2020.3024145},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/NimaTaherinejad-DesignAndTest.pdf},
  key		= {selfaware,eml,cdl},
  pages		= {1-1}
}

@InProceedings{	  wendt_cognitive_2020,
  address	= {United Kingdom (Online)},
  title		= {Cognitive {Architectures} for {Process} {Monitoring} - an
		  {Analysis}},
  isbn		= {978-1-72814-963-9},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/AlexanderWendt-INDIN.pdf},
  language	= {English},
  booktitle	= {Proceedings of the 18th {IEEE} {International}
		  {Conference} on {Industrial} {Informatics}},
  author	= {Wendt, Alexander and Kollmann, Stefan and Bratukhin,
		  Aleksey and Estaji, Alireza and Sauter, Thilo and Jantsch,
		  Axel},
  month		= jul,
  year		= {2020},
  pages		= {167--173}
}

@InProceedings{	  wendt_speeding_2020,
  address	= {Singapore (Online)},
  title		= {Speeding up {Common} {Hyperparameter} {Optimization}
		  {Methods} by a {Two}-{Phase}-{Search}},
  isbn		= {978-1-72815-413-8},
  abstract	= {Hyperparameter search concerns everybody who works with
		  machine learning. We compare publicly available
		  hyperparameter searches on four datasets. We develop
		  metrics to measure the performance of hyperparameter
		  searches across datasets of different sizes as well as
		  machine learning algorithms. Further, we propose a method
		  of speeding up the search by using subsets of data. Results
		  show that random search performs well compared to Bayesian
		  methods and that a combined search can speed up the search
		  by a factor of 5.},
  language	= {EN},
  booktitle	= {Proceedings of {IECON} 2020 {The} 46th {Annual}
		  {Conference} of the {IEEE} {Industrial} {Electronics}
		  {Society}},
  publisher	= {IEEE IES},
  author	= {Wendt, Alexander and Wuschnig, Marco and Lechner, Martin},
  year		= {2020},
  pages		= {517--522},
  url		= {https://www.researchgate.net/publication/344767663_Speeding_up_Common_Hyperparameter_Optimization_Methods_by_a_Two-Phase-Search}
}

@Article{	  wess:2021a,
  author	= {M. {Wess} and M. {Ivanov} and C. {Unger} and A. {Nookala}
		  and A. {Wendt} and A. {Jantsch}},
  journal	= {IEEE Access},
  title		= {ANNETTE: Accurate Neural Network Execution Time Estimation
		  With Stacked Models},
  year		= 2021,
  volume	= 9,
  pages		= {3545-3556},
  key		= {eml,cdl},
  abstract	= {With new accelerator hardware for Deep Neural Networks
		  (DNNs), the computing power for Artificial Intelligence
		  (AI) applications has increased rapidly. However, as DNN
		  algorithms become more complex and optimized for specific
		  applications, latency requirements remain challenging, and
		  it is critical to find the optimal points in the design
		  space. To decouple the architectural search from the target
		  hardware, we propose a time estimation framework that
		  allows for modeling the inference latency of DNNs on
		  hardware accelerators based on mapping and layer-wise
		  estimation models. The proposed methodology extracts a set
		  of models from micro-kernel and multi-layer benchmarks and
		  generates a stacked model for mapping and network execution
		  time estimation. We compare estimation accuracy and
		  fidelity of the generated mixed models, statistical models
		  with the roofline model, and a refined roofline model for
		  evaluation. We test the mixed models on the ZCU102 SoC
		  board with Xilinx Deep Neural Network Development Kit
		  (DNNDK) and Intel Neural Compute Stick 2 (NCS2) on a set of
		  12 state-of-the-art neural networks. It shows an average
		  estimation error of 3.47\% for the DNNDK and 7.44\% for the
		  NCS2, outperforming the statistical and analytical layer
		  models for almost all selected networks. For a randomly
		  selected subset of 34 networks of the NASBench dataset, the
		  mixed model reaches fidelity of 0.988 in Spearman’s $\rho
		  $ rank correlation coefficient metric.},
  keywords	= {Hardware;Computational modeling;Estimation;Benchmark
		  testing;Biological system modeling;Computer
		  architecture;Tools;Analytical models;estimation;neural
		  network hardware},
  doi		= {10.1109/ACCESS.2020.3047259},
  url		= {http://jantsch.se/AxelJantsch/papers/2021/MatthiasWess-IEEEAccess.pdf},
  issn		= {2169-3536}
}

@MastersThesis{	  worndle:2020a,
  author	= {Rudolf W\"{o}rndle},
  title		= {Continual Domain-Incremental Learning for Object Detection
		  },
  school	= {TU Graz},
  year		= 2020,
  key		= {cdl}
}

@MastersThesis{	  wuschnig:2020a,
  author	= {Marco Wuschnig},
  title		= {Auswertung verschiedener Methoden der
		  Hyperparameteroptimierung in Machine Learning},
  school	= {TU Wien},
  year		= 2020,
  key		= {cdl},
  type		= {Bachelor's Thesis},
  url		= {https://publik.tuwien.ac.at/files/publik_295896.pdf}
}

@MastersThesis{	  zahra:2020a,
  author	= {Anam Zahra},
  title		= {Autonomous Vehicle Self-localization in Noisy
		  Environments},
  school	= {TU Graz},
  year		= 2020,
  key		= {cdl}
}

@article{mirza2021robustness,
  title={Robustness of Object Detectors in Degrading Weather Conditions},
  author={Mirza, Muhammad Jehanzeb and Buerkle, Cornelius and Jarquin, Julio and Opitz, Michael and Oboril, Fabian and Scholl, Kay-Ulrich and Bischof, Horst},
  journal={International Conference on Intelligent Transportation Systems},
  year={2021}
}

@inproceedings{he2018amc,
  title={Automated Pruning of Neural Networks for Mobile Applications},
  author={Andreas Glinserer and Martin Lechner and Alexander Wendt},
  booktitle={To be published in Proceedings of IEEE International Conference on Industrial Informatics (INDIN)},
  year={2021}
}


@article{DBLP:journals/corr/abs-2106-08795,
  author    = {Muhammad Jehanzeb Mirza and
               Cornelius B{\"{u}}rkle and
               Julio Jarquin and
               Michael Opitz and
               Fabian Oboril and
               Kay{-}Ulrich Scholl and
               Horst Bischof},
  title     = {Robustness of Object Detectors in Degrading Weather Conditions},
  journal   = {CoRR},
  volume    = {abs/2106.08795},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.08795},
  archivePrefix = {arXiv},
  eprint    = {2106.08795},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08795.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

