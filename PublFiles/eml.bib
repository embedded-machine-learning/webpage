
@InProceedings{	  9290876,
  author	= {S. {Holly} and A. {Wendt} and M. {Lechner}},
  booktitle	= {2020 11th International Green and Sustainable Computing
		  Workshops (IGSC)},
  title		= {Profiling Energy Consumption of Deep Neural Networks on
		  NVIDIA Jetson Nano},
  year		= {2020},
  volume	= {},
  number	= {},
  pages		= {1-6},
  abstract	= {Improving the capabilities of embedded devices and
		  accelerators for Deep Neural Networks (DNN) leads to a
		  shift from cloud to edge computing. Especially for
		  battery-powered systems, intelligent energy management is
		  critical. In this work, we provide a measurement base for
		  power estimation on NVIDIA Jetson devices. We analyze the
		  effects of different CPU and GPU settings on power
		  consumption, latency, and energy for complete DNNs as well
		  as for individual layers. Furthermore, we provide optimal
		  settings for minimal power and energy consumption for an
		  NVIDIA Jetson Nano.},
  keywords	= {Power measurement;Sensors;Graphics processing units;Power
		  demand;Neural networks;Hardware;Energy
		  consumption;power;energy;latency;NVIDIA;Jetson;deep neural
		  network;profiling},
  doi		= {10.1109/IGSC51522.2020.9290876},
  issn		= {},
  month		= {Oct}
}

@Article{	  bellman:2020a,
  author	= {Kerstin Bellman and Nikil Dutt and Lukas Esterle and
		  Andreas Herkersdorf and Axel Jantsch and C. Landauer and P.
		  R. Lewis and M. Platzner and N. TaheriNejad and K.
		  Tammem\"{a}e},
  journal	= {ACM Transactions on Cyber-Physical Systems},
  title		= {Self-aware Cyber-Physical Systems},
  year		= 2020,
  key		= {selfaware,eml},
  pages		= {1-24},
  address	= {New York, NY, USA},
  volume	= {4},
  number	= {4},
  issn		= {2378-962X},
  url		= {https://doi.org/10.1145/3375716},
  doi		= {10.1145/3375716},
  abstract	= {In this article, we make the case for the new class of
		  Self-aware Cyber-physical Systems. By bringing together the
		  two established fields of cyber-physical systems and
		  self-aware computing, we aim at creating systems with
		  strongly increased yet managed autonomy, which is a main
		  requirement for many emerging and future applications and
		  technologies. Self-aware cyber-physical systems are
		  situated in a physical environment and constrained in their
		  resources, and they understand their own state and
		  environment and, based on that understanding, are able to
		  make decisions autonomously at runtime in a
		  self-explanatory way. In an attempt to lay out a research
		  agenda, we bring up and elaborate on five key challenges
		  for future self-aware cyber-physical systems: (i) How can
		  we build resource-sensitive yet self-aware systems? (ii)
		  How to acknowledge situatedness and subjectivity? (iii)
		  What are effective infrastructures for implementing
		  self-awareness processes? (iv) How can we verify self-aware
		  cyber-physical systems and, in particular, which guarantees
		  can we give? (v) What novel development processes will be
		  required to engineer self-aware cyber-physical systems? We
		  review each of these challenges in some detail and
		  emphasize that addressing all of them requires the system
		  to make a comprehensive assessment of the situation and a
		  continual introspection of its own state to sensibly
		  balance diverse requirements, constraints, short-term and
		  long-term objectives. Throughout, we draw on three examples
		  of cyber-physical systems that may benefit from
		  self-awareness: a multi-processor system-on-chip, a Mars
		  rover, and an implanted insulin pump. These three very
		  different systems nevertheless have similar
		  characteristics: limited resources, complex unforeseeable
		  environmental dynamics, high expectations on their
		  reliability, and substantial levels of risk associated with
		  malfunctioning. Using these examples, we discuss the
		  potential role of self-awareness in both highly complex and
		  rather more simple systems, and as a main conclusion we
		  highlight the need for research on above listed topics.},
  month		= {June},
  articleno	= {38},
  numpages	= {26},
  keywords	= {Self-awareness, guarantees, development processes,
		  verification, subjectivity, situatedness,
		  resource-sensitive, organizational infrastructure,
		  cyber-physical systems}
}

@InProceedings{	  colucci:2021a,
  author	= {Alessio Colucci and D\'avid Juh\'asz and Martin Mosbeck
		  and Alberto Marchisio and Semeen Rehman and Manfred
		  Kreutzer and GuÌˆnther Nadbath and Axel Jantsch and
		  Muhammad Shafique },
  title		= { {MLComp}: A Methodology for Machine Learning-based
		  Performance Estimation and Adaptive Selection of
		  {Pareto}-Optimal Compiler Optimization Sequences },
  key		= {eml},
  booktitle	= {Proceedings of the Design, Automation and Test in Europe
		  Conference and Exhibition },
  year		= 2021,
  month		= {March},
  url		= {http://jantsch.se/AxelJantsch/papers/2021/DavidJuhasz-DATE.pdf}
}

@InProceedings{	  fruhwirth:2020a,
  title		= {Towards Data-driven Multi-target Tracking for Autonomous
		  Driving},
  abstract	= {We investigate the potential of recurrent neural networks
		  (RNNs) to improve traditional on-line multi-target tracking
		  of traffic participants from an ego-vehicle perspective. To
		  this end, we build a modular tracking framework, based on
		  interacting multiple models (IMM) and unscented Kalman
		  filters (UKF). Following the tracking-by-detection
		  paradigm, we leverage geometric target properties provided
		  by publicly available 3D object detectors. We then train
		  and integrate two RNNs: A state pre-diction network
		  replaces hand-crafted motion mod-els in our filters and a
		  data association network finds detection-to-track
		  assignment probabilities. In our extensive evaluation on
		  the publicly available KITTI dataset we show that our
		  trained models achieve competitive results and are
		  significantly more robust in the case of unreliable object
		  defection.},
  author	= {Christian Fruhwirth-Reisinger and Georg Krispel and Horst
		  Possegger and Horst Bischof},
  year		= {2020},
  key		= {eml},
  language	= {English},
  pages		= {27--36},
  booktitle	= {Proceedings of the 25th Computer Vision Winter Workshop
		  (CVWW)},
  publisher	= {Slovenian Pattern Recognition Society},
  address	= {Slovenia}
}

@InProceedings{	  haas2021shunt,
  title		= {Neural Network Compression Through Shunt Connections and
		  Knowledge Distillation for Semantic Segmentation Problems},
  author	= {Bernhard Haas and Alexander Wendt and Axel Jantsch and
		  Matthias Wess},
  booktitle	= {to be published in Proceedings of AIAI 2021, 17th
		  International Conference on Artificial Intelligence
		  Applications and Innovations},
  volume	= {1},
  year		= {2021}
}

@InProceedings{	  haas:2021a,
  author	= { Bernhard Haas and Alexander Wendt and Axel Jantsch and
		  Matthias Wess },
  title		= {Neural Network Compression Through Shunt Connections and
		  Knowledge Distillation for Semantic Segmentation Problems
		  },
  key		= {eml},
  booktitle	= {17th International Conference on Artificial Intelligence
		  Applications and Innovations (AIAI)},
  month		= {June},
  year		= 2021
}

@Article{	  hoffmann:2020a,
  author	= {Henrik {Hoffmann} and Axel {Jantsch} and Nikil D. {Dutt}},
  journal	= {Proceedings of the IEEE},
  title		= {Embodied Self-Aware Computing Systems},
  year		= 2020,
  pages		= {1-20},
  key		= {selfaware,eml},
  keywords	= {Control;embedded systems (ESs);machine learning
		  (ML);self-aware computing.},
  doi		= {10.1109/JPROC.2020.2977054},
  issn		= {1558-2256},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/HankHoffmann-IEEEProceedings.pdf}
}

@InProceedings{	  krispel2020fuseseg,
  title		= {FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal
		  Data},
  author	= {Krispel, Georg and Opitz, Michael and Waltner, Georg and
		  Possegger, Horst and Bischof, Horst},
  booktitle	= {The IEEE Winter Conference on Applications of Computer
		  Vision},
  pages		= {1874--1883},
  year		= {2020}
}

@Article{	  mozelli:2021a,
  author	= {Amid Mozelli and Nima Taherinejad and Axel Jantsch},
  title		= {A Study on Confidence: an Unsupervised Multi-Agent Machine
		  Learning Experiment },
  journal	= {IEEE Design \& Test of Computers },
  year		= 2021,
  key		= {eml,selfaware},
  issn		= { 2168-2356 },
  doi		= { 10.1109/MDAT.2021.3078341 }
}

@Misc{		  presentation:2020a,
  key		= {ml, eml},
  author	= {Axel Jantsch },
  title		= {Embedded Machine Learning},
  howpublished	= {Invited Presentation at the Summer School on {AI} Enabled
		  Mobility },
  month		= {September},
  year		= 2020,
  annote	= {Graz, Austria},
  url		= {http://jantsch.se/AxelJantsch/presentations/2020/AI-EnabledMobility-SummerSchool.pdf}
}

@Misc{		  presentation:2020b,
  key		= {ml, eml},
  author	= {Horst Bischof },
  title		= {Computer Vision for Detection and Tracking},
  howpublished	= {Invited Presentation at the Summer School on {AI} Enabled
		  Mobility },
  month		= {September},
  year		= 2020,
  annote	= {Graz, Austria}
}

@Article{	  raina:2021a,
  author	= { Priyanka Raina and Cliff Young },
  title		= { Best Papers From Hot Chips },
  journal	= {IEEE Micro},
  year		= 2021,
  key		= {eml},
  volume	= 41,
  number	= 2,
  month		= {March-April},
  note		= {Special issue on the best papers from Hot Chips 32, 2020
		  }
}

@Article{	  shallari:2021a,
  author	= {Shallari, Irida and Sanchez Leal, Isaac and Krug, Silvia
		  and Jantsch, Axel and O'Nils, Mattias},
  journal	= {IEEE Access},
  title		= {{Design space exploration on IoT node: Trade-offs in
		  processing and communication}},
  year		= 2021,
  key		= {eml},
  doi		= {10.1109/ACCESS.2021.3074875},
  issn		= {2169-3536}
}

@Article{	  taherinejad:2020a,
  author	= {N. {TaheriNejad} and A. {Herkersdorf} and A. {Jantsch}},
  journal	= {IEEE Design Test},
  title		= {Autonomous Systems, Trust and Guarantees},
  year		= 2020,
  issn		= {2168-2356},
  doi		= { 10.1109/MDAT.2020.3024145},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/NimaTaherinejad-DesignAndTest.pdf},
  key		= {selfaware,eml},
  pages		= {1-1}
}

@Article{	  torres:2021a,
  author	= {Jos\'{e} F. Torres and Dalil Hadjout and Abderrazak Sebaa
		  and Francisco Mart\'{i}nez-\'{A}lvarez and and Alicia
		  Troncoso },
  title		= {Deep Learning for Time Series Forecasting: A Survey},
  journal	= {Big Data},
  year		= 2021,
  key		= {eml},
  volume	= 9,
  number	= 1,
  month		= {February},
  doi		= {10.1089/big.2020.0159}
}

@InProceedings{	  wendt_cognitive_2020,
  address	= {United Kingdom (Online)},
  title		= {Cognitive {Architectures} for {Process} {Monitoring} - an
		  {Analysis}},
  isbn		= {978-1-72814-963-9},
  url		= {http://jantsch.se/AxelJantsch/papers/2020/AlexanderWendt-INDIN.pdf},
  language	= {English},
  booktitle	= {Proceedings of the 18th {IEEE} {International}
		  {Conference} on {Industrial} {Informatics}},
  author	= {Wendt, Alexander and Kollmann, Stefan and Bratukhin,
		  Aleksey and Estaji, Alireza and Sauter, Thilo and Jantsch,
		  Axel},
  month		= jul,
  year		= {2020},
  pages		= {167--173}
}

@InProceedings{	  wendt_speeding_2020,
  address	= {Singapore (Online)},
  title		= {Speeding up {Common} {Hyperparameter} {Optimization}
		  {Methods} by a {Two}-{Phase}-{Search}},
  isbn		= {978-1-72815-413-8},
  abstract	= {Hyperparameter search concerns everybody who works with
		  machine learning. We compare publicly available
		  hyperparameter searches on four datasets. We develop
		  metrics to measure the performance of hyperparameter
		  searches across datasets of different sizes as well as
		  machine learning algorithms. Further, we propose a method
		  of speeding up the search by using subsets of data. Results
		  show that random search performs well compared to Bayesian
		  methods and that a combined search can speed up the search
		  by a factor of 5.},
  language	= {EN},
  booktitle	= {Proceedings of {IECON} 2020 {The} 46th {Annual}
		  {Conference} of the {IEEE} {Industrial} {Electronics}
		  {Society}},
  publisher	= {IEEE IES},
  author	= {Wendt, Alexander and Wuschnig, Marco and Lechner, Martin},
  year		= {2020},
  pages		= {517--522},
  url		= {https://www.researchgate.net/publication/344767663_Speeding_up_Common_Hyperparameter_Optimization_Methods_by_a_Two-Phase-Search}
}

@Article{	  wess:2021,
  author	= {M. {Wess} and M. {Ivanov} and C. {Unger} and A. {Nookala}
		  and A. {Wendt} and A. {Jantsch}},
  journal	= {IEEE Access},
  title		= {ANNETTE: Accurate Neural Network Execution Time Estimation
		  With Stacked Models},
  year		= {2021},
  volume	= {9},
  number	= {},
  pages		= {3545-3556},
  abstract	= {With new accelerator hardware for Deep Neural Networks
		  (DNNs), the computing power for Artificial Intelligence
		  (AI) applications has increased rapidly. However, as DNN
		  algorithms become more complex and optimized for specific
		  applications, latency requirements remain challenging, and
		  it is critical to find the optimal points in the design
		  space. To decouple the architectural search from the target
		  hardware, we propose a time estimation framework that
		  allows for modeling the inference latency of DNNs on
		  hardware accelerators based on mapping and layer-wise
		  estimation models. The proposed methodology extracts a set
		  of models from micro-kernel and multi-layer benchmarks and
		  generates a stacked model for mapping and network execution
		  time estimation. We compare estimation accuracy and
		  fidelity of the generated mixed models, statistical models
		  with the roofline model, and a refined roofline model for
		  evaluation. We test the mixed models on the ZCU102 SoC
		  board with Xilinx Deep Neural Network Development Kit
		  (DNNDK) and Intel Neural Compute Stick 2 (NCS2) on a set of
		  12 state-of-the-art neural networks. It shows an average
		  estimation error of 3.47% for the DNNDK and 7.44% for the
		  NCS2, outperforming the statistical and analytical layer
		  models for almost all selected networks. For a randomly
		  selected subset of 34 networks of the NASBench dataset, the
		  mixed model reaches fidelity of 0.988 in Spearmanâ€™s $\rho
		  $ rank correlation coefficient metric.},
  keywords	= {Hardware;Computational modeling;Estimation;Benchmark
		  testing;Biological system modeling;Computer
		  architecture;Tools;Analytical models;estimation;neural
		  network hardware},
  doi		= {10.1109/ACCESS.2020.3047259},
  issn		= {2169-3536},
  month		= {}
}

@Article{	  wess:2021a,
  author	= {M. {Wess} and M. {Ivanov} and C. {Unger} and A. {Nookala}
		  and A. {Wendt} and A. {Jantsch}},
  journal	= {IEEE Access},
  title		= {ANNETTE: Accurate Neural Network Execution Time Estimation
		  With Stacked Models},
  year		= 2021,
  volume	= 9,
  pages		= {3545-3556},
  key		= {eml},
  abstract	= {With new accelerator hardware for Deep Neural Networks
		  (DNNs), the computing power for Artificial Intelligence
		  (AI) applications has increased rapidly. However, as DNN
		  algorithms become more complex and optimized for specific
		  applications, latency requirements remain challenging, and
		  it is critical to find the optimal points in the design
		  space. To decouple the architectural search from the target
		  hardware, we propose a time estimation framework that
		  allows for modeling the inference latency of DNNs on
		  hardware accelerators based on mapping and layer-wise
		  estimation models. The proposed methodology extracts a set
		  of models from micro-kernel and multi-layer benchmarks and
		  generates a stacked model for mapping and network execution
		  time estimation. We compare estimation accuracy and
		  fidelity of the generated mixed models, statistical models
		  with the roofline model, and a refined roofline model for
		  evaluation. We test the mixed models on the ZCU102 SoC
		  board with Xilinx Deep Neural Network Development Kit
		  (DNNDK) and Intel Neural Compute Stick 2 (NCS2) on a set of
		  12 state-of-the-art neural networks. It shows an average
		  estimation error of 3.47% for the DNNDK and 7.44% for the
		  NCS2, outperforming the statistical and analytical layer
		  models for almost all selected networks. For a randomly
		  selected subset of 34 networks of the NASBench dataset, the
		  mixed model reaches fidelity of 0.988 in Spearmanâ€™s $\rho
		  $ rank correlation coefficient metric.},
  keywords	= {Hardware;Computational modeling;Estimation;Benchmark
		  testing;Biological system modeling;Computer
		  architecture;Tools;Analytical models;estimation;neural
		  network hardware},
  doi		= {10.1109/ACCESS.2020.3047259},
  issn		= {2169-3536}
}
